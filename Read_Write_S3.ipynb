{
    "cells": [
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "<h3>Read &amp; Write Content From Amazon S3</h3>"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "#Replace Accesskey with your amazon AccessKey and Secret with amazon secret\nhconf = sc._jsc.hadoopConfiguration()  \nhconf.set(\"fs.s3a.access.key\", \"XXXXXXXXXX\")\nhconf.set(\"fs.s3a.secret.key\", \"XXXXXXXXXX\")", 
            "outputs": [], 
            "execution_count": 3
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "spark = SparkSession.builder.getOrCreate()\ndf_data_1 = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load('s3a://charlesbuckets31/FolderA/users.csv')\ndf_data_1.take(5)", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "[Row(id=u'10001', name=u'Tony'),\n Row(id=u'10002', name=u'Mike'),\n Row(id=u'10003', name=u'Pat'),\n Row(id=u'10004', name=u'Chris'),\n Row(id=u'10005', name=u'Paco')]"
                    }, 
                    "execution_count": 5
                }
            ], 
            "execution_count": 5
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "df_data_1.printSchema()", 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "root\n |-- id: string (nullable = true)\n |-- name: string (nullable = true)\n\n", 
                    "output_type": "stream"
                }
            ], 
            "execution_count": 6
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "df_data_1.write.save(\"s3a://charlesbuckets31/FolderB/users.parquet\")", 
            "outputs": [], 
            "execution_count": 11
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "df_data_2 = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat')\\\n  .option('header', 'true')\\\n  .load('s3a://charlesbuckets31/FolderB/users.parquet')\ndf_data_2.take(5)", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "[Row(id=u'10001', name=u'Tony'),\n Row(id=u'10002', name=u'Mike'),\n Row(id=u'10003', name=u'Pat'),\n Row(id=u'10004', name=u'Chris'),\n Row(id=u'10005', name=u'Paco')]"
                    }, 
                    "execution_count": 12
                }
            ], 
            "execution_count": 12
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 2 with Spark 2.0", 
            "name": "python2-spark20", 
            "language": "python"
        }, 
        "language_info": {
            "file_extension": ".py", 
            "nbconvert_exporter": "python", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }, 
            "version": "2.7.11", 
            "mimetype": "text/x-python", 
            "pygments_lexer": "ipython2", 
            "name": "python"
        }
    }, 
    "nbformat_minor": 0, 
    "nbformat": 4
}